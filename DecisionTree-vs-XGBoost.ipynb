### 1.Data Exploring
pip install dmba
# Import libraries
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report
from dmba import plotDecisionTree, classificationSummary, regressionSummary

from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

from itertools import chain
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
# Read graduate data and show the head rows of that
Graduate = pd.read_csv(r"C:\Users\ASUS\Desktop\CSULB\Machin Learning for Business Analytics IS670\Project2\StudentGraduate.csv")
Graduate.head()
# Read emplyee data and show the head rows of that
Class = pd.read_csv(r"C:\Users\ASUS\Desktop\CSULB\Machin Learning for Business Analytics IS670\Project2\StudentClass.csv")
Class.head()
#Changing the name of StudentID TO ID
Class.rename(columns={'StudentID': 'ID'}, inplace=True)
#Combining the two files into one dataframe based on EmployeeID
df = pd.merge(Graduate, Class, on = 'ID')
df.head()
df.shape
df.dtypes
df.describe().T
sns.countplot(x ='Major',data=Graduate,hue="GraduateONTime")
plt.legend(['not on time','on time'])
print('Graduate on time by Major \n')
sns.countplot(x ='sex',data=Graduate,hue="GraduateONTime")
plt.legend(['not on time','on time'])

print('Graduate on time by GRADE \n')
plt.figure(figsize=(18, 10))
corr = df.corr()
sns.heatmap(corr,cmap ="YlGnBu",cbar=True,annot=True,fmt=".1f")
plt.show()
corrdf = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))
corrdf = corrdf.unstack().reset_index()
corrdf.columns = ['feature1', 'feature2', 'Correlation']
corrdf.dropna(subset = ['Correlation'], inplace = True)
corrdf['Correlation'] = round(corrdf['Correlation'], 2)
corrdf['Correlation'] = abs(corrdf['Correlation'])
matrix= corrdf.sort_values(by = 'Correlation', ascending = False)
high_corr = matrix[matrix['Correlation']>0.2]
high_corr
### 2. Data prepration
df['ClassDate'] = pd.to_datetime(df['ClassDate'])
# creates semesters from each class
df['Semester'] = df.ClassDate.dt.year.astype(str) + " S" + \
    np.where(df.ClassDate.dt.quarter.gt(2),1,2).astype(str)
df
# drop the last two semesters for each student 
df = df.drop(df.groupby('ID').tail(2).index, axis=0)
df
df['GraduateONTime'] = df['GraduateONTime'].replace([0,1], ['Not on time','on time'])
df
#Droppping unnecessary columns
df_drop=df.drop(columns=['ClassMeetTime','ClassDate'])
df_drop
df_drop.dtypes
# Change categorical variables to "category"
df_drop['Major'] = df_drop['Major'].astype('category')
df_drop['GRADE'] = df_drop['GRADE'].astype('category')
df_drop['ClassWeekDay'] = df_drop['ClassWeekDay'].astype('category')
df_drop['ClassTitle'] = df_drop['ClassTitle'].astype('category')
df_drop['Semester'] = df_drop['Semester'].astype('category')
df_drop['GraduateONTime'] = df_drop['GraduateONTime'].astype('category')
df_drop.dtypes
#determine columns with blank/missing data
df_drop.isnull().sum()
#Replace null fields with median because they are all integer and check again
df_drop.fillna(df.median(), inplace=True)
df_drop.isnull().sum()
#Check columns with just 1 unique value
uniquecount = df_drop.nunique()
print(uniquecount)
# Create dummy variables
df_drop_dummy = pd.get_dummies(df_drop, columns=['Major','GRADE','ClassWeekDay','ClassTitle','Semester'],drop_first=True)
df_drop_dummy
# Apply standardization
numeric_variables = df_drop_dummy[['HS_ENGLISH','ID','HS_MATH','HS_HISTORY','HS_LAB_SCIENCES','HS_FOREIGN_LANGUAGE','HS_ART','HS_ELECTIVES','HS_GPA','sex','WRITINGScore']]
scaler_s = StandardScaler().fit(numeric_variables)
standard_variables = scaler_s.transform(numeric_variables)
print(standard_variables)
df_drop_dummy[['HS_ENGLISH','ID','HS_MATH','HS_HISTORY','HS_LAB_SCIENCES','HS_FOREIGN_LANGUAGE','HS_ART','HS_ELECTIVES','HS_GPA','sex','WRITINGScore']] = standard_variables
df_drop_dummy
# Partition the data
target = df_drop_dummy['GraduateONTime']
predictors = df_drop_dummy.drop(['GraduateONTime'], axis=1)
predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size = 0.3, random_state = 0)
print(predictors_train.shape, predictors_test.shape, target_train.shape, target_test.shape)
# Examine the porportion of target variable for training data set
print(target_train.value_counts(normalize= True))
# Examine the porportion of target variable for testing data set
print(target_test.value_counts(normalize= True))
### 3.XGBoost
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
                         n_estimators=100)

bdt.fit(predictors_train, target_train)
classificationSummary(target_train, bdt.predict(predictors_train))
classificationSummary(target_test, bdt.predict(predictors_test))
### 4.Decision Tree Model

smallClassTree = DecisionTreeClassifier(max_depth=30, min_samples_split=20, min_impurity_decrease=0.01)
smallClassTree.fit(predictors_train, target_train)

plotDecisionTree(smallClassTree, feature_names=predictors_train.columns)
classificationSummary(target_train, smallClassTree.predict(predictors_train))
classificationSummary(target_test, smallClassTree.predict(predictors_test))
